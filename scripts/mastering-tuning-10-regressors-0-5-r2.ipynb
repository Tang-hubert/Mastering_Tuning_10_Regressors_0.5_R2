{"cells":[{"cell_type":"markdown","metadata":{},"source":["<div style=\"display:fill;\n","            border-radius:15px;\n","            background-color:skyblue;\n","            font-size:210%;\n","            font-family:sans-serif;\n","            letter-spacing:0.5px;\n","            padding:10px;\n","            color:white;\n","            border-style: solid;\n","            border-color: black;\n","            text-align:center;\">\n","<b>\n"," üë®‚Äçüî¨ In-Depth 10 Regressors to Predict Data Science Salary üí∞</b></div>"]},{"cell_type":"markdown","metadata":{},"source":["This notebook will show **10 diversified regressors** made up with parametric, non-parametric and ensemble learning methods to predict on data science salary. It will also show **Nested Grid Search** to tuned the pre-processor along with the estimator parameters. Besides, several visualisation idioms, such as **Bar of Pie and Residual Plots** will also be depicted. The best model pipeline can be retrieved at the end to make a final prediction. Hope you enjoy reading this, if you find this notebook useful, please **upvote and comment**. Thank you.\n","\n","Author: Morris Lee <br>\n","Date: 4-9-2022"]},{"cell_type":"markdown","metadata":{},"source":["#### [1.0 Preprocessing Part](#1.0)\n","* [1.1 Import Packages and Define Useful Functions](#1.1)\n","* [1.2 Inspect Duplications](#1.2)\n","* [1.3 üé® Inspect Value Counts](#1.3)\n","* [1.4 Get GDP Per Capita](#1.4)\n","* [1.5 Add New Columns](#1.5)\n","* [1.6 üé® Bar of Pie Chart](#1.6)\n","* [1.7 Reduce Dimension of Job Title](#1.7)\n","* [1.8 Distinguish Categorical and Numerical](#1.8)\n","* [1.9 One Hot Encode](#1.9)\n","* [1.10 üé® Plot Distributions](#1.10)\n","    \n","#### [2.0 Modelling Part](#2.0)\n","* [2.1 Import Modelling Packages](#2.1)\n","* [2.2 Define Nested Grid Search Functions](#2.2)\n","* [2.3 Define Residual Plotting Functions](#2.3)\n","* [2.4 ‚≠ê Modelling - 10 Models - Training + Evaluations](#2.4)\n","* [2.5 Concatenate Results](#2.5)\n","* [2.6 Get Overall Best Results](#2.6)\n","* [2.7 Make a Final Prediction](#2.7)"]},{"cell_type":"markdown","metadata":{},"source":["# <b>1.0 <span style='color:red'>|</span> Pre-Processing Part</b> <a class=\"anchor\" id=\"1.0\"></a>"]},{"cell_type":"markdown","metadata":{},"source":["# <b>1.1 <span style='color:red'>|</span> Import Packages and Define Useful Functions </b> <a class=\"anchor\" id=\"1.1\"></a>"]},{"cell_type":"markdown","metadata":{},"source":["Let's import several useful functions that will use in this notebook:\n","1. def vc - to pretty show the value counts of a column\n","2. def shape - to pretty show the dimension of a dataframe\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-12-18T15:27:41.686802Z","iopub.status.busy":"2023-12-18T15:27:41.686406Z","iopub.status.idle":"2023-12-18T15:27:41.721661Z","shell.execute_reply":"2023-12-18T15:27:41.720415Z","shell.execute_reply.started":"2023-12-18T15:27:41.686771Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import pycountry\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import ConnectionPatch\n","from matplotlib.patches import Circle\n","import matplotlib\n","import seaborn as sns\n","\n","def shape(df,df_name):\n","    print(f'STATUS: Dimension of \"{df_name}\" = {df.shape}')\n","\n","def vc(df, column, r=False):\n","    vc_df = df.reset_index().groupby([column]).size().to_frame('count')\n","    vc_df['percentage (%)'] = vc_df['count'].div(sum(vc_df['count'])).mul(100)\n","    vc_df = vc_df.sort_values(by=['percentage (%)'], ascending=False)\n","    if r:\n","        return vc_df\n","    else:\n","        print(f'STATUS: Value counts of \"{column}\"...')\n","        display(vc_df)\n","        \n","df = pd.read_csv(\"./data/ds_salaries.csv\")\n","df.drop('Unnamed: 0', axis=1, inplace=True)\n","display(df.head())\n","df.info()"]},{"cell_type":"markdown","metadata":{},"source":["# <b>1.2 <span style='color:red'>|</span> Inspect Duplications </b> <a class=\"anchor\" id=\"1.2\"></a>"]},{"cell_type":"markdown","metadata":{},"source":["There are 42 duplicated rows in the dataframe, let's remove them"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-18T15:27:41.724589Z","iopub.status.busy":"2023-12-18T15:27:41.724125Z","iopub.status.idle":"2023-12-18T15:27:41.749577Z","shell.execute_reply":"2023-12-18T15:27:41.748515Z","shell.execute_reply.started":"2023-12-18T15:27:41.724545Z"},"trusted":true},"outputs":[],"source":["num_duplicated = len(df[df.duplicated()])\n","print(f'STATUS: There are {num_duplicated} duplicated rows')\n","shape(df,'df')\n","df = df.drop_duplicates()\n","shape(df,'After removing duplicates')\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["# <b>1.3 <span style='color:red'>|</span> Inspect Value Counts </b> <a class=\"anchor\" id=\"1.3\"></a>"]},{"cell_type":"markdown","metadata":{},"source":["Let's study the value counts of the dataframe. In order to easy knowing the labels meaning, the shortforms have replaced to more explicit meaningful words. Asides of showing the value counts, we can plot the donut chart... Sounds delicious :p"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-18T15:27:41.751683Z","iopub.status.busy":"2023-12-18T15:27:41.751073Z","iopub.status.idle":"2023-12-18T15:27:42.319644Z","shell.execute_reply":"2023-12-18T15:27:42.318787Z","shell.execute_reply.started":"2023-12-18T15:27:41.751642Z"},"trusted":true},"outputs":[],"source":["df = df.replace({'EN': 'Entry-level', 'SE': 'Senior-level', 'EX':'Expert', 'MI':'Mid-level',\n","           'PT': 'Part-time', 'FT':'Full-time', 'CT':'Contract', 'FL':'Freelance'})\n","\n","plt.style.use('ggplot')\n","\n","def pie(df, column):\n","\n","    fig, axs = plt.subplots(nrows = 2, ncols = 2)\n","    fig = matplotlib.pyplot.gcf()\n","    fig.subplots_adjust(wspace=0.1)\n","    fig.set_size_inches(15, 13)\n","    fig.suptitle(f\"Donut Charts\", fontsize=20,fontweight='bold')\n","    \n","    counter = 0\n","    for i in range(2):\n","        for j in range(2):\n","            target = column[counter]\n","            # Pie chart, where the slices will be ordered and plotted counter-clockwise:\n","            labels = df[target].value_counts().index.tolist()\n","            sizes = np.rint(df[target].value_counts().values/ df[target].value_counts().values.sum() *100)\n","            explode = tuple(np.zeros(len(labels))+0.1)\n","\n","            axs[i,j].pie(sizes, labels=labels, autopct='%1.1f%%', radius=2, explode = explode)\n","            axs[i,j].axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n","            axs[i,j].set_title(f'{target}', fontsize=18, fontname=\"Arial\",fontweight='bold')\n","\n","            #draw circle\n","            centre_circle = Circle((0,0),1,fc='white')\n","            axs[i,j].add_patch(centre_circle)\n","            \n","            counter+=1\n","\n","    return plt.show()\n","\n","pie(df, ['work_year','experience_level','employment_type','company_size'])\n","\n","\n","\n","categorical = ['work_year', 'experience_level','employment_type','job_title','salary_currency','employee_residence','company_location','company_size']\n","for col in categorical:\n","    vc(df,col)"]},{"cell_type":"markdown","metadata":{},"source":["# <b>1.4 <span style='color:red'>|</span> Get GDP Per Capita </b> <a class=\"anchor\" id=\"1.4\"></a>"]},{"cell_type":"markdown","metadata":{},"source":["Here is a interesting one, gdp per capita information has obtained from other kaggle dataset. The objective is to merge back to the main dataframe to make it more meaningful."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-18T15:27:42.321297Z","iopub.status.busy":"2023-12-18T15:27:42.320937Z","iopub.status.idle":"2023-12-18T15:28:02.495138Z","shell.execute_reply":"2023-12-18T15:28:02.491580Z","shell.execute_reply.started":"2023-12-18T15:27:42.321265Z"},"trusted":true},"outputs":[],"source":["url = 'https://raw.githubusercontent.com/k-w-lee/kaggle-file/main/gdp_per_capita.csv'\n","gdp_per_capita = pd.read_csv(url)\n","gdp_per_capita = gdp_per_capita[['Country','2018']]\n","gdp_per_capita.columns = ['country','gdp_per_capita']\n","gdp_per_capita['gdp_per_capita']=pd.to_numeric(gdp_per_capita['gdp_per_capita'],errors='coerce')\n","gdp_per_capita"]},{"cell_type":"markdown","metadata":{},"source":["# <b>1.5 <span style='color:red'>|</span> Add New Columns </b> <a class=\"anchor\" id=\"1.5\"></a>"]},{"cell_type":"markdown","metadata":{},"source":["Here is a bunch of pre-processing concentrate in one kernel. To summarise, 6 new columns have been added to further process the data. Eventually, several redundant columns were then dropped\n"," "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-18T15:28:02.496182Z","iopub.status.idle":"2023-12-18T15:28:02.496875Z","shell.execute_reply":"2023-12-18T15:28:02.496647Z","shell.execute_reply.started":"2023-12-18T15:28:02.496623Z"},"trusted":true},"outputs":[],"source":["shape(df,'df before added columns')\n","# to get country name for employee residence\n","df['employee_residence_name'] = df.apply(lambda x: pycountry.countries.get(alpha_2 =x['employee_residence']).name \\\n","                                         if (pycountry.countries.get(alpha_2 =x['employee_residence']) is not None) \\\n","                                         else 'None' ,axis=1)\n","print(\"STATUS: Added column employee_residence_name\")\n","\n","# to get country name for company residence\n","df['company_location_name'] = df.apply(lambda x: pycountry.countries.get(alpha_2 =x['company_location']).name \\\n","                                         if (pycountry.countries.get(alpha_2 =x['company_location']) is not None) \\\n","                                         else 'None' ,axis=1)\n","print(\"STATUS: Added column company_location_name\")\n","\n","# to get company's country gdp per capita\n","df2 = df.merge(gdp_per_capita, how='left', left_on='company_location_name', right_on = 'country')\n","df2.rename(columns = {'gdp_per_capita': 'gdp_per_capita_company'}, inplace=True)\n","df2.drop('country', inplace=True, axis=1)\n","print(\"STATUS: Added column company's country gdp per capita\")\n","\n","# to get residence's country gdp per capita\n","df2 = df2.merge(gdp_per_capita, how='left', left_on='employee_residence_name', right_on = 'country')\n","df2.drop('country', inplace=True, axis=1)\n","df2.rename(columns = {'gdp_per_capita': 'gdp_per_capita_residence'}, inplace=True)\n","print(\"STATUS: Added column residence's country gdp per capita\")\n","\n","# is the working country same as residence country?\n","df2['same_working_country'] = df2.apply(lambda x:  'Local Worker' if x['company_location_name'] == x['employee_residence_name'] else 'Expatriate', axis=1)\n","print(\"STATUS: Added column same_working_country\")\n","\n","# is the working country gdp per capita same as residence country?\n","df2['went_high_went_low_gdp_capita'] = df2.apply(lambda x:  'Went Higher GDP per Capita' if x['gdp_per_capita_company'] > x['gdp_per_capita_residence'] else ('Went Lower GDP per Capita' if x['gdp_per_capita_company'] < x['gdp_per_capita_residence'] else 'Same'), axis=1)\n","print(\"STATUS: Added column went_high_went_low_gdp_capita\")\n","\n","shape(df2,'Before drop NA')\n","# dropna for those country code can't be captured\n","df2=df2.dropna()\n","\n","pycountry.currencies.get(alpha_3='ARS')\n","\n","# drop redundant columns\n","to_drop = ['employee_residence_name', 'company_location_name','employee_residence',\\\n","           'company_location','salary_currency','salary']\n","df2 = df2.drop(to_drop, axis=1)\n","print(f\"STATUS: Dropped {to_drop}\")\n","\n","shape(df2,'After drop NA')\n","df2"]},{"cell_type":"markdown","metadata":{},"source":["# <b>1.6 <span style='color:red'>|</span> Bar of Pie Chart </b> <a class=\"anchor\" id=\"1.6\"></a>\n"]},{"cell_type":"markdown","metadata":{},"source":["This section will answer questions like: Is there any expatriate in the dataset? If Yes, are they going to a country with higher GDP per Capita or Lower? How is the distribution? Let Bar of Pie answer you this question. :)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-18T15:28:02.499479Z","iopub.status.idle":"2023-12-18T15:28:02.499893Z","shell.execute_reply":"2023-12-18T15:28:02.499725Z","shell.execute_reply.started":"2023-12-18T15:28:02.499706Z"},"trusted":true},"outputs":[],"source":["# make figure and assign axis objects\n","\n","def pie_bar(ratios_pie, labels_pie, ratios_bar, labels_bar_tuple, colors_list, color_bar,bar_title,pie_title):\n","    fig = plt.figure(figsize=(9, 10.0625))\n","    ax1 = fig.add_subplot(121)\n","    ax2 = fig.add_subplot(122)\n","    fig.subplots_adjust(wspace=0)\n","\n","    # pie chart parameters\n","    ratios = ratios_pie\n","    labels = labels_pie\n","    explode = [0.1, 0]\n","    # rotate so that first wedge is split by the x-axis\n","    angle = -180 * ratios[0]\n","    ax1.pie(ratios, autopct='%1.1f%%', startangle=angle,\n","            labels=labels, explode=explode, colors = colors_list)\n","    ax1.set_title(pie_title, fontsize=20, fontname=\"Arial\",fontweight='bold')\n","    # bar chart parameters\n","    xpos = 0\n","    bottom = 0\n","    ratios = ratios_bar\n","    width = .2\n","    colors = color_bar\n","\n","    for j in range(len(ratios)):\n","        height = ratios[j]\n","        ax2.bar(xpos, height, width, bottom=bottom, color=colors[j])\n","        ypos = bottom + ax2.patches[j].get_height() / 2\n","        bottom += height\n","        ax2.text(xpos, ypos, \"%d%%\" % (ax2.patches[j].get_height() * 100),\n","                 ha='center')\n","\n","    ax2.set_title(bar_title, fontsize=20, fontname=\"Arial\",fontweight='bold')\n","    ax2.legend(labels_bar_tuple, loc='upper right')\n","    ax2.axis('off')\n","    ax2.set_xlim(- 2.5 * width, 2.5 * width)\n","\n","    # use ConnectionPatch to draw lines between the two plots\n","    # get the wedge data\n","    theta1, theta2 = ax1.patches[0].theta1, ax1.patches[0].theta2\n","    center, r = ax1.patches[0].center, ax1.patches[0].r\n","    bar_height = sum([item.get_height() for item in ax2.patches])\n","\n","    # draw top connecting line\n","    x = r * np.cos(np.pi / 180 * theta2) + center[0]\n","    y = np.sin(np.pi / 180 * theta2) + center[1]\n","    con = ConnectionPatch(xyA=(- width / 2, bar_height), xyB=(x, y),\n","                          coordsA=\"data\", coordsB=\"data\", axesA=ax2, axesB=ax1)\n","    con.set_color([0, 0, 0])\n","    con.set_linewidth(4)\n","    ax2.add_artist(con)\n","\n","    # draw bottom connecting line\n","    x = r * np.cos(np.pi / 180 * theta1) + center[0]\n","    y = np.sin(np.pi / 180 * theta1) + center[1]\n","    con = ConnectionPatch(xyA=(- width / 2, 0), xyB=(x, y), coordsA=\"data\",\n","                          coordsB=\"data\", axesA=ax2, axesB=ax1)\n","    con.set_color([0, 0, 0])\n","    ax2.add_artist(con)\n","    con.set_linewidth(4)\n","\n","    return plt.show()\n","\n","\n","labels_pie = df2.same_working_country.value_counts(normalize=True).index.tolist()\n","ratios_pie = df2.same_working_country.value_counts(normalize=True).tolist()\n","\n","# TO REVERSE THE PIE CHART TO CORRECT POSITION\n","ratios_pie.insert(0, ratios_pie.pop(1))\n","labels_pie.insert(0, labels_pie.pop(1))\n","\n","ratios_bar = df2[df2.same_working_country == 'Expatriate'].went_high_went_low_gdp_capita.value_counts(normalize=True).tolist()\n","labels_bar_tuple = tuple(df2[df2.same_working_country =='Expatriate'].went_high_went_low_gdp_capita.value_counts(normalize=True).index.tolist())\n","color_bar = ['lightgreen', 'pink','skyblue']\n","color_pie = ['lime', 'lightcoral']\n","pie_bar(ratios_pie, labels_pie, ratios_bar, labels_bar_tuple, color_pie, color_bar, \"GDP Per Capita in the Working Country\",'Is the Employee Local Worker?')"]},{"cell_type":"markdown","metadata":{},"source":["# <b>1.7 <span style='color:red'>|</span> Reduce Dimension of Job Title </b> <a class=\"anchor\" id=\"1.7\"></a>\n"]},{"cell_type":"markdown","metadata":{},"source":["The 'job_title' column is further pre-process to reduce number of unique values. I discover there are many values not necessarily to be unique, hence they are merged together. The detailes are summarised in kernel below."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-18T15:28:02.501233Z","iopub.status.idle":"2023-12-18T15:28:02.501632Z","shell.execute_reply":"2023-12-18T15:28:02.501453Z","shell.execute_reply.started":"2023-12-18T15:28:02.501435Z"},"trusted":true},"outputs":[],"source":["df2 = df2.replace({'ML Engineer': 'Machine Learning Engineer', \n","                   'BI Data Analyst' : 'Big Data Engineer', \n","                   'Data Analytics Engineer': 'Data Analyst', \n","                   'Head of Machine Learning':'Machine Learning Manager', \n","                   'Lead Machine Learning Engineer':'Machine Learning Manager',\n","                   'Staff Data Scientist':'Data Scientist',\n","                   'Big Data Architect':'Big Data Engineer',\n","                   'Data Analytics Lead':'Data Analytics Manager', \n","                   'Lead Data Scientist':'Head of Data Science',\n","                   'Machine Learning Infrastructure Engineer':'Machine Learning Engineer',\n","                   'Data Specialist':'Data Scientist',\n","                   'Marketing Data Analyst':'Data Analyst',\n","                   'Finance Data Analyst':'Data Analyst',\n","                   'Financial Data Analyst':'Data Analyst',\n","                   'Product Data Analyst':'Data Analyst',\n","                   '3D Computer Vision Researcher':'Computer Vision Engineer',\n","                   'Computer Vision Software Engineer':'Computer Vision Engineer',\n","                   'NLP Engineer':'Data Scientist',\n","                   'Applied Machine Learning Scientist': 'Machine Learning Engineer', \n","                   'ETL Developer':'Data Architect','Principal Data Analyst':'Lead Data Analyst'})\n","\n","searchfor = ['Head', 'Lead', 'Manager','Director','Principal']\n","is_managerial = df2['job_title'].str.contains('|'.join(searchfor))\n","df2['is_managerial'] = np.where(is_managerial, True, False)\n","\n","display(df2.head())\n","vc(df2,'job_title')"]},{"cell_type":"markdown","metadata":{},"source":["# <b>1.8 <span style='color:red'>|</span> Distinguish Categorical and Numerical </b> <a class=\"anchor\" id=\"1.8\"></a>\n"]},{"cell_type":"markdown","metadata":{},"source":["To easy distinguish CATEGORICAL or NUMERICAL, we can use a function to calculate how many unique values in a column to decide. At here, I put if there is more than 10 unique values, then it will be considered as numerical. In fact, it will also included 'job_title' as numerical, which is incorrect. I purposely put so because later I would like to examine I should use OrdinalEncoding or OneHotEncoding to treat this column can generate a better result. So, I temporarily park it here first. The other categorical columns will be treated with OneHotEncoding. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-18T15:28:02.504116Z","iopub.status.idle":"2023-12-18T15:28:02.504949Z","shell.execute_reply":"2023-12-18T15:28:02.504679Z","shell.execute_reply.started":"2023-12-18T15:28:02.504649Z"},"trusted":true},"outputs":[],"source":["df3 = df2.copy()\n","\n","def get_num_cat_col(df, n):\n","    numerical_columns = []\n","    categorical_columns = []\n","    for col in df.columns:\n","        len_unique = len(df[col].unique())\n","        if len_unique <= n:\n","            categorical_columns.append(col)\n","        else:\n","            numerical_columns.append(col)\n","    return numerical_columns, categorical_columns\n","\n","num, cat = get_num_cat_col(df2, 10)\n","print('NUMERICAL', num , '\\nCATEGORICAL' ,cat)"]},{"cell_type":"markdown","metadata":{},"source":["# <b>1.9 <span style='color:red'>|</span> One Hot Encode </b> <a class=\"anchor\" id=\"1.9\"></a>\n"]},{"cell_type":"markdown","metadata":{},"source":["We can define a function to easy implement the one-hot-encoding process, that automatic rename, drop and join the encoded columns. Sound hassle free!"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-18T15:28:02.506436Z","iopub.status.idle":"2023-12-18T15:28:02.507258Z","shell.execute_reply":"2023-12-18T15:28:02.506976Z","shell.execute_reply.started":"2023-12-18T15:28:02.506947Z"},"trusted":true},"outputs":[],"source":["def one_hot_encode(df, column):\n","    # Get one hot encoding of columns B\n","    one_hot = pd.get_dummies(df[column]).add_prefix(f'{column}_')\n","    # Drop column as it is now encoded\n","    df = df.drop(column,axis = 1)\n","    print(f\"one hot encoded {column}\")\n","    # Join the encoded df\n","    df = df.join(one_hot)\n","    return df\n","\n","for col in cat:\n","    df3 = one_hot_encode(df3, col)\n","shape(df3,'After One Hot Encoded')\n","df3.head()"]},{"cell_type":"markdown","metadata":{},"source":["# <b>1.10 <span style='color:red'>|</span> Plot Distributions </b> <a class=\"anchor\" id=\"1.10\"></a>\n"]},{"cell_type":"markdown","metadata":{},"source":["Now, we want to plot the distribution of numerical columns. But remember to remove the 'job_title' first that temporarily parked there. \n","\n","After plotting, we notice that our target variable 'salary_in_usd' is skewed to the right. We can treat the skewness by using log transformation and the prediction can be reversed back. The log transformation is a reversible transforme so it is not a problem to use this to alter your target values, as it can be transformed back by using exponential function."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-18T15:28:02.508741Z","iopub.status.idle":"2023-12-18T15:28:02.509570Z","shell.execute_reply":"2023-12-18T15:28:02.509297Z","shell.execute_reply.started":"2023-12-18T15:28:02.509268Z"},"trusted":true},"outputs":[],"source":["def vis_dist(df, col):\n","    variable = df[col].values\n","    ax = sns.displot(variable)\n","    plt.title(f'Distribution of {col}', fontsize=20, fontname=\"Arial\",fontweight='bold')\n","    plt.xlabel(f'{col}')\n","    return plt.show()\n","\n","num.remove('job_title')\n","for col in num:\n","    vis_dist(df3, col)\n"]},{"cell_type":"markdown","metadata":{},"source":["# <b>2.0 <span style='color:red'>|</span> Modelling </b> <a class=\"anchor\" id=\"2.0\"></a>\n"]},{"cell_type":"markdown","metadata":{},"source":["# <b>2.1 <span style='color:red'>|</span> Import Modelling Packages </b> <a class=\"anchor\" id=\"2.1\"></a>\n"]},{"cell_type":"markdown","metadata":{},"source":["Let's import a bunch of packages for modelling"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-18T15:28:02.511042Z","iopub.status.idle":"2023-12-18T15:28:02.511834Z","shell.execute_reply":"2023-12-18T15:28:02.511575Z","shell.execute_reply.started":"2023-12-18T15:28:02.511547Z"},"trusted":true},"outputs":[],"source":["from sklearn.experimental import enable_halving_search_cv\n","from sklearn.model_selection import HalvingGridSearchCV\n","from sklearn.ensemble import RandomForestRegressor\n","import seaborn as sns\n","from sklearn.model_selection import ParameterGrid\n","from sklearn.pipeline import make_pipeline\n","from sklearn.linear_model import Ridge\n","from sklearn.compose import TransformedTargetRegressor\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import Normalizer\n","from sklearn.preprocessing import OrdinalEncoder\n","from sklearn.compose import make_column_transformer\n","from sklearn.preprocessing import OneHotEncoder\n","import scipy as sp\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.neighbors import KNeighborsRegressor\n","import matplotlib\n","from sklearn.linear_model import Lasso\n","from sklearn.svm import SVR\n","from sklearn.ensemble import GradientBoostingRegressor\n","from sklearn.ensemble import HistGradientBoostingRegressor\n","from sklearn.ensemble import AdaBoostRegressor\n","from sklearn.linear_model import LinearRegression\n"]},{"cell_type":"markdown","metadata":{},"source":["# <b>2.2 <span style='color:red'>|</span> Define Nested Grid Search Functions </b> <a class=\"anchor\" id=\"2.2\"></a>"]},{"cell_type":"markdown","metadata":{},"source":["Here is the illustration of how we can find a unique combination of PRE-PROCESSORS by using PARAM GRID. The idea is to FIND the BEST TREATMENT for your data with the objective to maximise the metric, such as R2. Then, in each loop, we will also tune the regressor estimator by using Halving Grid Search CV. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-18T15:28:02.513316Z","iopub.status.idle":"2023-12-18T15:28:02.514136Z","shell.execute_reply":"2023-12-18T15:28:02.513855Z","shell.execute_reply.started":"2023-12-18T15:28:02.513827Z"},"trusted":true},"outputs":[],"source":["tuned_model = None\n","target_transformers = [TransformedTargetRegressor(regressor=tuned_model, \n","                           func=np.log10, \n","                           inverse_func=sp.special.exp10),\n","                       TransformedTargetRegressor(regressor=tuned_model)]\n","\n","categorical_transformers = [OneHotEncoder(handle_unknown='ignore'), \n","                            OrdinalEncoder()]\n","\n","scaling_transformers = [Normalizer(), \n","                        StandardScaler()]\n","\n","param_grid = {'target_transformers':target_transformers,\n","              'categorical_transformers':categorical_transformers,\n","              'scaling_transformers':scaling_transformers}\n","grid = ParameterGrid(param_grid)\n","\n","for n, para in enumerate(grid, start=1):\n","    print(n)\n","    print(para)\n","    print()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-18T15:28:02.515647Z","iopub.status.idle":"2023-12-18T15:28:02.516459Z","shell.execute_reply":"2023-12-18T15:28:02.516186Z","shell.execute_reply.started":"2023-12-18T15:28:02.516158Z"},"trusted":true},"outputs":[],"source":["target_transformers = [TransformedTargetRegressor(regressor=tuned_model, \n","                           func=np.log10, \n","                           inverse_func=sp.special.exp10),\n","                       TransformedTargetRegressor(regressor=tuned_model)]\n","\n","categorical_transformers = [OneHotEncoder(handle_unknown='ignore'), \n","                            OrdinalEncoder()]\n","\n","scaling_transformers = [Normalizer(), \n","                        StandardScaler()]\n","\n","param_grid = {'target_transformers':target_transformers,\n","              'categorical_transformers':categorical_transformers,\n","              'scaling_transformers':scaling_transformers}\n","grid = ParameterGrid(param_grid)\n","\n","def assembling_model(categorical_transform, scale_transform,  \\\n","                     X,  y, reg, param_distributions, tune):\n","        \n","    preprocessor = make_column_transformer(\n","        (categorical_transform, categorical_columns),\n","        (scale_transform, numerical_columns),\n","    )\n","    \n","    x_transform = preprocessor.fit_transform(X)\n","    X_train, X_test, y_train, y_test = train_test_split(x_transform, y, random_state=42)\n","    if tune:\n","        search = HalvingGridSearchCV(reg, param_distributions, random_state=42)\n","        search.fit(X_train, y_train)\n","        best_model = search.best_estimator_\n","        best_param = search.best_params_\n","    else:\n","        search = reg\n","        search.fit(X_train, y_train)\n","        best_model = reg\n","        best_param = None\n","    return best_model, X_train, X_test, y_train, y_test, best_param, preprocessor\n","\n","def tuning_whole_algorithm(X,  y, reg, param_distributions, grid, tune=True):\n","    result_list = []\n","    for para in tqdm(grid):\n","        tuned_model, X_train, X_test, y_train, y_test, parameters, preprocessor_pipe = assembling_model(para['categorical_transformers'], para['scaling_transformers'], \\\n","                         X,  y, reg, param_distributions, tune)\n","        \n","        para['target_transformers'].regressor = tuned_model\n","        model = para['target_transformers']\n","        \n","        # storing pipeline information\n","        pipeline_cache = make_pipeline(preprocessor_pipe, model)\n","        \n","        model.fit(X_train, y_train)\n","        prediction_test = model.predict(X_test)\n","        model_text_list=[]; metric_list=[]; score_list=[] ; param_list=[] ; preprocessors_list=[] ; pipelines_list = []\n","\n","        # create list of metric to be examined\n","        metric_functions = [r2_score, r2_score, mean_squared_error,mean_squared_error,mean_absolute_error]\n","        metric_functions_text = ['R_Squared', 'Adj_R_Squared', 'MSE','RMSE','MAE']\n","\n","        # for loop of each of the 5 metrics\n","        for metric_function, metric_function_text in zip(metric_functions, metric_functions_text):\n","            if metric_function_text == 'Adj_R_Squared':\n","                Adj_r2 = 1 - (1-r2_score(y_test, prediction_test)) * (len(y)-1)/(len(y)-X.shape[1]-1)\n","                model_text_list.append(type(model.regressor).__name__); metric_list.append(metric_function_text); score_list.append(Adj_r2); param_list.append(parameters); preprocessors_list.append(para); pipelines_list.append(pipeline_cache)\n","            elif metric_function_text == 'RMSE':\n","                rmse = mean_squared_error(y_test, prediction_test, squared=False)\n","                model_text_list.append(type(model.regressor).__name__); metric_list.append(metric_function_text); score_list.append(rmse); param_list.append(parameters) ; preprocessors_list.append(para); pipelines_list.append(pipeline_cache)\n","            else:\n","                model_text_list.append(type(model.regressor).__name__); metric_list.append(metric_function_text); score_list.append(metric_function(y_test, prediction_test)); param_list.append(parameters) ; preprocessors_list.append(para); pipelines_list.append(pipeline_cache)\n","\n","        d = {'model':model_text_list,'preprocessors':preprocessors_list ,'parameters': param_list ,'metric': metric_list, 'test predict score': score_list, 'Pipelines': pipelines_list}\n","        df = pd.DataFrame(data=d)\n","        result_list.append(df)\n","    df2 = pd.concat(result_list).reset_index(drop=True)\n","    return df2"]},{"cell_type":"markdown","metadata":{},"source":["# <b>2.3 <span style='color:red'>|</span> Define Residual Plotting Functions </b> <a class=\"anchor\" id=\"2.3\"></a>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-18T15:28:02.517925Z","iopub.status.idle":"2023-12-18T15:28:02.518715Z","shell.execute_reply":"2023-12-18T15:28:02.518460Z","shell.execute_reply.started":"2023-12-18T15:28:02.518432Z"},"trusted":true},"outputs":[],"source":["def residual(model,model_name,X_train,X_test,y_train,y_test):\n","    model.fit(X_train, y_train)\n","    prediction_train = model.predict(X_train)\n","    prediction_test = model.predict(X_test)\n","    \n","    # PERFORMANCE METRICS\n","    mse_test = mean_squared_error(y_test, prediction_test, squared=True)\n","    rmse_test = mean_squared_error(y_test, prediction_test, squared=False)\n","    mse_train = mean_squared_error(y_train, prediction_train, squared=True)\n","    rmse_train = mean_squared_error(y_train, prediction_train, squared=False)\n","    \n","    # RESIDUAL FOR ACTUAL AND LOGGED\n","    residual_train = y_train - prediction_train\n","    residual_test = y_test - prediction_test\n","    \n","    fig, axs = plt.subplots(nrows = 1, ncols = 2)\n","    fig = matplotlib.pyplot.gcf()\n","    fig.subplots_adjust(wspace=0.1)\n","    fig.set_size_inches(12, 5)\n","    fig.suptitle(f\"Residual of Calibrated {model_name}\", fontsize=14,fontweight='bold')\n","\n","    axs[0].sharex(axs[1])\n","    axs[0].sharey(axs[1])\n","\n","    axs[0].scatter(x = prediction_train,y = residual_train, alpha=0.1,color='red',label='Train Set')\n","    axs[0].set_title(f'Training Set',fontweight='bold')\n","    axs[0].set_xlabel('Predicted Values')\n","    axs[0].set_ylabel('Residual')\n","    yabs_max = abs(max(axs[0].get_ylim(), key=abs))\n","    axs[0].axhline(y=0, color='black', linestyle='--', label='Zero Residual')\n","    axs[0].legend()\n","    \n","    axs[1].scatter(x = prediction_test,y = residual_test, alpha=0.1,color='blue',label='Test Set')\n","    axs[1].set_title(f'Testing Set',fontweight='bold')\n","    axs[1].set_xlabel('Predicted Values')\n","    yabs_max = abs(max(axs[1].get_ylim(), key=abs))\n","    axs[1].axhline(y=0, color='black', linestyle='--', label='Zero Residual')\n","    axs[1].legend()\n","\n","    props = dict(boxstyle='square', facecolor='whitesmoke', alpha=1, pad=0.5)\n","    axs[0].text(0.6, 0.75, f\"MSE = {mse_train:.2f} \\nRMSE = {rmse_train:.2f}\", transform=axs[0].transAxes, fontsize=10,\n","        verticalalignment='top', bbox=props)\n","    axs[1].text(0.6, 0.75, f\"MSE = {mse_test:.2f} \\nRMSE = {rmse_test:.2f}\", transform=axs[1].transAxes, fontsize=10,\n","        verticalalignment='top', bbox=props)\n","    return plt.show()\n","\n","\n","def get_best_model(df):\n","    df_t = df[df.metric== 'Adj_R_Squared']\n","    bestmodel = df_t.loc[df_t['test predict score'].idxmax()].Pipelines\n","    model_name = df_t.loc[df_t['test predict score'].idxmax()].model\n","    return bestmodel, model_name\n","\n","def get_best_result(df_result):\n","    df_result_t = df_result[df_result.metric== 'R_Squared']\n","    r2_df = df_result_t.loc[df_result_t['test predict score'].idxmax()].to_frame().T\n","\n","    df_result_t = df_result[df_result.metric== 'Adj_R_Squared']\n","    adjr2_df = df_result_t.loc[df_result_t['test predict score'].idxmax()].to_frame().T\n","    \n","    df_result_t = df_result[df_result.metric== 'MSE']\n","    mse_df = df_result_t.loc[df_result_t['test predict score'].idxmin()].to_frame().T\n","\n","    df_result_t = df_result[df_result.metric== 'RMSE']\n","    rmse_df = df_result_t.loc[df_result_t['test predict score'].idxmin()].to_frame().T\n","    \n","    df_result_t = df_result[df_result.metric== 'MAE']\n","    mae_df = df_result_t.loc[df_result_t['test predict score'].idxmin()].to_frame().T\n","    return pd.concat([r2_df,adjr2_df,mse_df,rmse_df,mae_df])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-18T15:28:02.520177Z","iopub.status.idle":"2023-12-18T15:28:02.520973Z","shell.execute_reply":"2023-12-18T15:28:02.520708Z","shell.execute_reply.started":"2023-12-18T15:28:02.520679Z"},"trusted":true},"outputs":[],"source":["X = df3.drop('salary_in_usd', axis=1)\n","y = df3.salary_in_usd.values\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n","\n","categorical_columns = ['job_title']\n","numerical_columns = X.columns.tolist()\n","numerical_columns.remove('job_title')\n","numerical_columns"]},{"cell_type":"markdown","metadata":{},"source":["# <b>2.4 <span style='color:red'>|</span> Modelling - Training + Evaluations </b> <a class=\"anchor\" id=\"2.4\"></a>\n"]},{"cell_type":"markdown","metadata":{},"source":["### <b>2.4.1 <span style='color:red'>|</span> LinearRegression </b> <a class=\"anchor\" id=\"1.1\"></a>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-18T15:28:02.522420Z","iopub.status.idle":"2023-12-18T15:28:02.523222Z","shell.execute_reply":"2023-12-18T15:28:02.522942Z","shell.execute_reply.started":"2023-12-18T15:28:02.522915Z"},"trusted":true},"outputs":[],"source":["reg = LinearRegression()\n","param_distributions = None\n","pd.set_option('display.max_colwidth', 200)\n","df_result_lr = tuning_whole_algorithm(X,  y, reg, param_distributions, grid, tune=False)\n","\n","# visualise residual\n","bestmodel, model_name = get_best_model(df_result_lr)\n","residual(bestmodel, model_name,X_train,X_test,y_train,y_test)\n","\n","# show result\n","get_best_result(df_result_lr)"]},{"cell_type":"markdown","metadata":{},"source":["### <b>2.4.2 <span style='color:red'>|</span> Ridge </b> <a class=\"anchor\" id=\"1.1\"></a>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-18T15:28:02.524654Z","iopub.status.idle":"2023-12-18T15:28:02.525464Z","shell.execute_reply":"2023-12-18T15:28:02.525189Z","shell.execute_reply.started":"2023-12-18T15:28:02.525161Z"},"trusted":true},"outputs":[],"source":["reg = Ridge(alpha=.5)\n","\n","param_distributions = None\n","\n","df_result_ridge = tuning_whole_algorithm(X,  y, reg, param_distributions, grid, tune=False)\n","\n","# visualise residual\n","bestmodel, model_name = get_best_model(df_result_ridge)\n","residual(bestmodel, model_name,X_train,X_test,y_train,y_test)\n","\n","# show result\n","get_best_result(df_result_ridge)"]},{"cell_type":"markdown","metadata":{},"source":["### <b>2.4.3 <span style='color:red'>|</span> Lasso </b> <a class=\"anchor\" id=\"1.1\"></a>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-18T15:28:02.526871Z","iopub.status.idle":"2023-12-18T15:28:02.527679Z","shell.execute_reply":"2023-12-18T15:28:02.527418Z","shell.execute_reply.started":"2023-12-18T15:28:02.527390Z"},"trusted":true},"outputs":[],"source":["reg = Lasso(alpha=0.1, tol = 0.2)\n","\n","param_distributions = None\n","\n","df_result_lasso = tuning_whole_algorithm(X,  y, reg, param_distributions, grid, tune=False)\n","\n","# visualise residual\n","bestmodel, model_name = get_best_model(df_result_lasso)\n","residual(bestmodel, model_name,X_train,X_test,y_train,y_test)\n","\n","# show result\n","get_best_result(df_result_lasso)"]},{"cell_type":"markdown","metadata":{},"source":["### <b>2.4.4 <span style='color:red'>|</span> KNeighborsRegressor </b> <a class=\"anchor\" id=\"1.1\"></a>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-18T15:28:02.529109Z","iopub.status.idle":"2023-12-18T15:28:02.529906Z","shell.execute_reply":"2023-12-18T15:28:02.529645Z","shell.execute_reply.started":"2023-12-18T15:28:02.529617Z"},"trusted":true},"outputs":[],"source":["reg = KNeighborsRegressor()\n","\n","param_distributions = {'n_neighbors': [5, 7, 9, 13], 'weights': ['uniform', 'distance']}\n","df_result_knn = tuning_whole_algorithm(X,  y, reg, param_distributions, grid, tune=True)\n","\n","# visualise residual\n","bestmodel, model_name = get_best_model(df_result_knn)\n","residual(bestmodel, model_name,X_train,X_test,y_train,y_test)\n","\n","# show result\n","get_best_result(df_result_knn)"]},{"cell_type":"markdown","metadata":{},"source":["### <b>2.4.5 <span style='color:red'>|</span> SVR </b> <a class=\"anchor\" id=\"1.1\"></a>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-18T15:28:02.531366Z","iopub.status.idle":"2023-12-18T15:28:02.532174Z","shell.execute_reply":"2023-12-18T15:28:02.531888Z","shell.execute_reply.started":"2023-12-18T15:28:02.531860Z"},"trusted":true},"outputs":[],"source":["reg = SVR()\n","param_distributions = {'kernel': ['rbf','poly'],'C':[50,100,200,300,400]}\n","\n","df_result_svr = tuning_whole_algorithm(X,  y, reg, param_distributions, grid)\n","\n","# visualise residual\n","bestmodel, model_name = get_best_model(df_result_svr)\n","residual(bestmodel, model_name,X_train,X_test,y_train,y_test)\n","\n","# show result\n","get_best_result(df_result_svr)"]},{"cell_type":"markdown","metadata":{},"source":["### <b>2.4.6 <span style='color:red'>|</span> DecisionTreeRegressor </b> <a class=\"anchor\" id=\"1.1\"></a>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-18T15:28:02.533595Z","iopub.status.idle":"2023-12-18T15:28:02.534394Z","shell.execute_reply":"2023-12-18T15:28:02.534126Z","shell.execute_reply.started":"2023-12-18T15:28:02.534097Z"},"trusted":true},"outputs":[],"source":["reg = DecisionTreeRegressor(random_state=42)\n","\n","param_distributions = {'max_depth': [3, 5, None], 'min_samples_split': [2, 3, 5]}\n","df_result_dtr = tuning_whole_algorithm(X,  y, reg, param_distributions, grid)\n","\n","# visualise residual\n","bestmodel, model_name = get_best_model(df_result_dtr)\n","residual(bestmodel, model_name,X_train,X_test,y_train,y_test)\n","\n","# show result\n","get_best_result(df_result_dtr)"]},{"cell_type":"markdown","metadata":{},"source":["### <b>2.4.7 <span style='color:red'>|</span> RandomForestRegressor </b> <a class=\"anchor\" id=\"1.1\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-18T15:28:02.535803Z","iopub.status.idle":"2023-12-18T15:28:02.536600Z","shell.execute_reply":"2023-12-18T15:28:02.536334Z","shell.execute_reply.started":"2023-12-18T15:28:02.536307Z"},"trusted":true},"outputs":[],"source":["reg = RandomForestRegressor(random_state=42)\n","param_distributions = {'max_depth': [3, 5, None], 'n_estimators': [100, 300]}\n","df_result_rfr = tuning_whole_algorithm(X,  y, reg, param_distributions, grid)\n","\n","# visualise residual\n","bestmodel, model_name = get_best_model(df_result_rfr)\n","residual(bestmodel, model_name,X_train,X_test,y_train,y_test)\n","\n","# show result\n","get_best_result(df_result_rfr)"]},{"cell_type":"markdown","metadata":{},"source":["### <b>2.4.8 <span style='color:red'>|</span> GradientBoostingRegressor </b> <a class=\"anchor\" id=\"1.1\"></a>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-18T15:28:02.538026Z","iopub.status.idle":"2023-12-18T15:28:02.538828Z","shell.execute_reply":"2023-12-18T15:28:02.538559Z","shell.execute_reply.started":"2023-12-18T15:28:02.538531Z"},"trusted":true},"outputs":[],"source":["reg = GradientBoostingRegressor(random_state=42)\n","param_distributions = {'learning_rate': [0.1, 0.2, 0.5]}\n","df_result_gbr = tuning_whole_algorithm(X,  y, reg, param_distributions, grid)\n","\n","# visualise residual\n","bestmodel, model_name = get_best_model(df_result_gbr)\n","residual(bestmodel, model_name,X_train,X_test,y_train,y_test)\n","\n","# show result\n","get_best_result(df_result_gbr)"]},{"cell_type":"markdown","metadata":{},"source":["### <b>2.4.9 <span style='color:red'>|</span> HistGradientBoostingRegressor </b> <a class=\"anchor\" id=\"1.1\"></a>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-18T15:28:02.540290Z","iopub.status.idle":"2023-12-18T15:28:02.541100Z","shell.execute_reply":"2023-12-18T15:28:02.540821Z","shell.execute_reply.started":"2023-12-18T15:28:02.540793Z"},"trusted":true},"outputs":[],"source":["param_distributions = {'learning_rate': [0.1, 0.2, 0.5]}\n","\n","reg = HistGradientBoostingRegressor(random_state=42)\n","df_result_hgb = tuning_whole_algorithm(X,  y, reg, param_distributions, grid)\n","\n","# visualise residual\n","bestmodel, model_name = get_best_model(df_result_hgb)\n","residual(bestmodel, model_name,X_train,X_test,y_train,y_test)\n","\n","# show result\n","get_best_result(df_result_hgb)"]},{"cell_type":"markdown","metadata":{},"source":["### <b>2.4.10 <span style='color:red'>|</span> AdaBoostRegressor </b> <a class=\"anchor\" id=\"1.1\"></a>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-18T15:28:02.542514Z","iopub.status.idle":"2023-12-18T15:28:02.543305Z","shell.execute_reply":"2023-12-18T15:28:02.543043Z","shell.execute_reply.started":"2023-12-18T15:28:02.543000Z"},"trusted":true},"outputs":[],"source":["reg = AdaBoostRegressor(random_state=42)\n","df_result_ada = tuning_whole_algorithm(X,  y, reg, None, grid, tune=False)\n","# visualise residual\n","bestmodel, model_name = get_best_model(df_result_ada)\n","residual(bestmodel, model_name,X_train,X_test,y_train,y_test)\n","\n","# show result\n","get_best_result(df_result_ada)"]},{"cell_type":"markdown","metadata":{},"source":["# <b>2.5 <span style='color:red'>|</span> Concatenate Results </b> <a class=\"anchor\" id=\"2.5\"></a>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-18T15:28:02.544719Z","iopub.status.idle":"2023-12-18T15:28:02.545532Z","shell.execute_reply":"2023-12-18T15:28:02.545251Z","shell.execute_reply.started":"2023-12-18T15:28:02.545224Z"},"trusted":true},"outputs":[],"source":["df_result = pd.concat([df_result_lr, df_result_ridge, df_result_lasso, \n","                       df_result_knn, df_result_svr, df_result_dtr,\n","                       df_result_rfr, df_result_hgb, df_result_gbr,\n","                       df_result_ada]).reset_index(drop=True)\n","df_result"]},{"cell_type":"markdown","metadata":{},"source":["# <b>2.6 <span style='color:red'>|</span> Get Overall Best Results </b> <a class=\"anchor\" id=\"2.6\"></a>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-18T15:28:02.546937Z","iopub.status.idle":"2023-12-18T15:28:02.547751Z","shell.execute_reply":"2023-12-18T15:28:02.547493Z","shell.execute_reply.started":"2023-12-18T15:28:02.547465Z"},"trusted":true},"outputs":[],"source":["get_best_result(df_result)"]},{"cell_type":"markdown","metadata":{},"source":["# <b>2.7 <span style='color:red'>|</span> Make a Final Prediction </b> <a class=\"anchor\" id=\"2.7\"></a>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-18T15:28:02.549191Z","iopub.status.idle":"2023-12-18T15:28:02.549985Z","shell.execute_reply":"2023-12-18T15:28:02.549718Z","shell.execute_reply.started":"2023-12-18T15:28:02.549690Z"},"trusted":true},"outputs":[],"source":["df_result_t = df_result[df_result.metric== 'Adj_R_Squared']\n","BEST_model = df_result_t.loc[df_result_t['test predict score'].idxmax()].Pipelines.fit(X_train, y_train)\n","\n","print('PREDICTED VALUES')\n","best_predict = BEST_model.predict(X_test)\n","pd.DataFrame({'Actual Y-Test Salary':y_test, 'Best Predicted Salary':best_predict})"]},{"cell_type":"markdown","metadata":{},"source":["As a conclusion, this notebook has shown 10 regressor made up with parametric, non-parametric and ensemble methods to predict on data science salary. It has also shown how to tuned the pre-processor along with the estimator parameters. The best model can be retrieved at the end to make a final prediction. Hope you enjoy reading this, if you like this notebook, please upvote and comment. Thank you.\n","\n","Author: Morris Lee <br>\n","Date: 4-9-2022"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":2268489,"sourceId":3806098,"sourceType":"datasetVersion"}],"dockerImageVersionId":30235,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":4}
